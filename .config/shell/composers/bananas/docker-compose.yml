services:
  # Automatic1111 service configuration
  automatic1111:
    build:
      context: ./automatic1111
      dockerfile: Dockerfile
    container_name: automatic1111
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${AUTOMATIC_GPU_DRIVER:-nvidia}
              count: ${AUTOMATIC_GPU_COUNT:-1}
              capabilities:
                - compute
                - gpu
                - utility
    networks:
      - bananas-network
    ports:
      - "${AUTOMATIC_PORT:-7860}:7860"
    restart: unless-stopped
    runtime: nvidia
    tty: true
    volumes:
      - ${AUTOMATIC_DATA_DIR:-${HOME}/.bananas/automatic1111}/extensions:/app/extensions
      - ${AUTOMATIC_DATA_DIR:-${HOME}/.bananas/automatic1111}/models:/app/models
      - ${AUTOMATIC_DATA_DIR:-${HOME}/.bananas/automatic1111}/outputs:/app/outputs
      - ${AUTOMATIC_DATA_DIR:-${HOME}/.bananas/automatic1111}/repositories:/app/repositories

  # ComfyUI service configuration
  comfyui:
    build:
      context: ./comfyui
      dockerfile: Dockerfile
    container_name: comfyui
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${AUTOMATIC_GPU_DRIVER:-nvidia}
              count: ${AUTOMATIC_GPU_COUNT:-1}
              capabilities:
                - compute
                - gpu
                - utility
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    networks:
      - bananas-network
    ports:
      - "${COMFYUI_PORT:-8188}:8188"
    runtime: nvidia
    volumes:
      - ${COMFYUI_DATA_DIR:-${HOME}/.bananas/comfyui}/custom_nodes:/app/custom_nodes
      - ${COMFYUI_DATA_DIR:-${HOME}/.bananas/comfyui}/models:/app/models
      - ${COMFYUI_DATA_DIR:-${HOME}/.bananas/comfyui}/output:/app/output
      - ${COMFYUI_DATA_DIR:-${HOME}/.bananas/comfyui}/workflows:/app/workflows

  # Ollama service configuration
  ollama:
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${AUTOMATIC_GPU_DRIVER:-nvidia}
              count: ${AUTOMATIC_GPU_COUNT:-1}
              capabilities:
                - compute
                - gpu
                - utility
    image: ollama/ollama:${OLLAMA_DOCKER_TAG:-latest}
    networks:
      - bananas-network
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    restart: unless-stopped
    runtime: nvidia
    tty: true
    volumes:
      - ${OLLAMA_DATA_DIR:-${HOME}/.bananas/ollama}:/root/.ollama

  # OpenWebUI service configuration
  open-webui:
    container_name: open-webui
    depends_on:
      - ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUID_SECRET_KEY=''
    extra_hosts:
      - host.docker.internal:host-gateway
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG:-main}
    networks:
      - bananas-network
    ports:
      - "${WEBUI_PORT:-3000}:8080"
    restart: unless-stopped
    volumes:
      - ${WEBUI_DATA_DIR:-${HOME}/.bananas/open-webui}:/app/backend/data

# Define a custom network for inter-service communication
networks:
  bananas-network:
    driver: bridge
